---
title: "Math 571A Homework 2"
author: "Nick Kappler"
output: pdf_document
header-includes: \usepackage{bbm} \usepackage{amsmath} \usepackage{tabularx}
---

##### 2.3.) Marketing Game

While the negative slope does indicate the stated relationship, the fact that the $p$-value is so large means we can't accept the results of the regression.  The conclusion should be that marketing dollars does not significantly affect sales.

##### 2.4.) 
GPA and ACT score 

###### a.
```{r}
data4 <- read.table("CH01PR19.txt")
names(data4) <-c('GPA','ACT')
attach(data4)
model4 = lm(GPA~ACT)

interval4 <- confint(model4,level=0.99)
interval4[2,]
```

The 99% confidence interval for the slope parameter is (`r interval4[2,1]`,`r interval4[2,2]`).  It does not include 0.  If the confidence interval contains 0, then the slope is not significant at the 1% level.

###### b.
Our test is as follows:
\begin{center}
\begin{tabular}{c r}
H$_0$:&$\beta_1 = 0$\\
H$_a$:&$\beta_1 \neq 0$\\
\end{tabular}
\end{center}

```{r}
alpha4 <- 0.01
summ4 <- summary(model4)
tval4 <- summ4$coefficients[2,3]
tstar4 <- qt(1-alpha4/2,model4$df)
```

The decision rule is:
\begin{tabular}{c c c}
if& $|t^*|\leq t(1-`r alpha4/2`, `r model4$df`)$,& Conclude $H_0$\\
if& $|t^*|> t(1-`r alpha4`/2, `r model4$df`)$,& Conclude $H_a$\\
\end{tabular}

For $\alpha = 0.01$, the critical t-value is $t(`r 1-alpha4/2`,`r model4$df`) = `r tstar4`$.  R gives $t^*=`r tval4`$; reject the null.  There is significant evidence that there is  linear relation between ACT score and GPA at the end of freshman year.

###### c. 
Calculate the probability :
\[
\mathbbm{P}[|t(118)|>t=`r tval4`]
\]
But, this is also stored in R:
```{r}
summ4$coefficients
```

So, we have $P = `r summ4$coefficients[2,4]`<\alpha=.01$.  The $P$-value is less than $\alpha$, so this calculation supports the result from part b.  Under the null hypothesis, the probability of observing a sample with a $t$ statistic as large or larger than what we calculated is less than the significance level.
`r detach(data4)`

##### 2.5.)
 Copier Maintenance

##### a.
This is just a confidence interval on $\beta_1$.
```{r}
alpha5 = 0.1
data5 <- read.table("CH01PR20.txt")
names(data5)<-c('Time','Copiers')
attach(data5)
model5 <- lm(Time~Copiers)
ints5 <- confint(model5,parm='Copiers',level=1-alpha5)
print(ints5)
```

So the 90\% confidence interval is $(`r ints5[1]`,`r ints5[2]`)$.  We can be 90\% confident that the increase in mean service time when the number of copiers serviced increases by 1 is between $`r ints5[1]`$ and $`r ints5[2]`$ hours $(`r ints5[1]*60`$ and $`r ints5[2]*60`$ minutes ).

##### b.
Our test is as follows:
\begin{center}
\begin{tabular}{c r}
H$_0$:&$\beta_1 = 0$\\
H$_a$:&$\beta_1 \neq 0$\\
\end{tabular}

\end{center}
The Decision Rule is:
\begin{tabular}{c c c}
if& $|t^*|\leq t(1-`r alpha5/2`, `r model5$df`),$& Conclude $H_0$\\
if& $|t^*|> t(1-`r alpha5/2`, `r model5$df`),$& Conclude $H_a$\\
\end{tabular}
```{r}
summ5 <- summary(model5)
tval5 <- summ5$coefficients[2,3]
tstar5 <- qt(1-alpha5/2,model5$df)
```

For $\alpha = 0.1$, the critical t-value is $t(`r 1-alpha5`,`r model5$df`) = `r tstar5`$.  R gives $t^*=`r tval5`$;   There is significant evidence that there is  linear relation between the mean service time and the number of copiers serviced.  R says the $P$-value is $`r summ5$coefficients[2,4]`$.. or, about 0.

###### c.
Yes the results are consistent.  I expect it (if everything is done correctly) because of the equivalence of confidence intervals and hypothesis tests.  The confidence interval does not contain 0, and I rejected the null hypothesis.

###### d.
```{r}
alpha5d <- .05
s5d <- summ5$coefficients[2,2]
tval5d <- (model5$coefficients[2]-14)/s5d
tstar5d <- qt(alpha5d,model5$df)
P5d <- pt(tval5d,df = model5$df)
```

Our test is as follows:
\begin{center}
\begin{tabular}{c r}
H$_0$:&$\beta_1 \geq 14$\\
H$_a$:&$\beta_1 < 14$\\
\end{tabular}

\end{center}
The Decision Rule is:
\begin{tabular}{c c c}
if& $t^*\geq t(`r alpha5d`, `r model5$df`),$& Conclude $H_0$\\
if& $t^*< t(`r alpha5d`, `r model5$df`),$& Conclude $H_a$\\
\end{tabular}

For $\alpha = `r alpha5d`$, the critical t-value for the one-sided test is $t(`r alpha5d`,`r model5$df`) = `r tstar5d`$.  R gives $t^*=`r tval5d`$.  We fail to reject the null hypothesis that $\beta_1 \geq 14$; there is insufficient evidence to conclude that the time to repair an additional copier requires on average less than 14 minutes of extra repair time.  The $P$-value is $`r P5d`$.

`r detach(data5)`

##### 2.13.)
Grade Point Average

```{r}
attach(data4)
alpha.13 = .025
conf.int.13 <-predict(model4,data.frame(ACT=28),interval='conf',level=0.95)
pred.int.13 <-predict(model4,data.frame(ACT=28),interval='pred',level=0.95)
n= length(ACT)
#Xh = seq(from=min(ACT),to=max(ACT),length=100)
Xh.13 = 28
Yh.13 = model4$coef[1]+Xh.13*model4$coef[2]
se.Yh.13 = sqrt(((Xh.13-mean(ACT))^2/((n-1)*var(ACT))+1/n)*summ4$sigma^2)
W.13 = sqrt(2*qf(1-alpha.13,2,n-2))

WHSu.13 = Yh.13 + 2*se.Yh.13
WHSl.13 = Yh.13 - 2*se.Yh.13
WHS.int.13 <-c(WHSl.13,WHSu.13)
#plot( WHSl ~ Xh, type='l', xlim=c(min(Xh),max(Xh)),ylim=c(0,4), xlab='', ylab='' )
#par(new = T)
#plot( WHSu ~ Xh, type='l', xlim=c(min(Xh),max(Xh)),ylim=c(0,4), xlab='X', ylab='E[Y]' )
#abline(model4$coef[1],model4$coef[2])
detach(data4)
```

###### a. 
The 95\% confidence interval is $(`r conf.int.13[2]`,`r conf.int.13[3]`)$.  The

###### b.
The 95\% prediction interval is $(`r pred.int.13[2]`,`r pred.int.13[3]`)$.

###### c.
The prediction interval is wider.  It should be, since we must account for not only variance from the prediction, that is $\sigma(\hat{Y_h})$, but also from the SLR model itself, that is $\sigma(\varepsilon_h)$.

###### d. 
The confidence band at $X_h=28$ (the interval) is $(`r WHS.int.13[1]`,`r WHS.int.13[2]`)$, just slightly wider than the confidence interval in part a.  We expect it to be wider because the factor $W$ in the formula must take into account the entire regression line, whereas the critical $t$ value in the confidence interval needs only to take into account the single point.

##### 2.24.)
Copier maintenance

######a. 
```{r}
anova.24 = anova(model5)
attach(data5)
```

Format of Table 2.2:
\begin{tabularx}{\textwidth}{|r| c |c| c| c}
\textbf{Source of Variation}&SS&df&MS&$\mathbbm{E}[MS]$\\
\hline
Regression&$`r anova.24[1,2]`$&$`r anova.24[1,1]`$&$`r anova.24[1,3]`$&$\sigma^2+\beta_1^2`r sum((Copiers-mean(Copiers))^2)`$\\
Error&$`r anova.24[2,2]`$&$`r anova.24[2,1]`$&$`r anova.24[2,3]`$&$\sigma^2$\\
\hline
Total&$`r anova.24[2,2]+anova.24[1,2]`$&$`r anova.24[1,1]+anova.24[2,1]`$&\\
\end{tabularx}

Not sure what is meant by 'additive,' but the bottom row is the sum of the top two rows.

Format of Table 2.3:
\begin{tabularx}{\textwidth}{|r|c| c| c| c}
\textbf{Source of Variation}&SS&df&MS&$\mathbbm{E}[MS]$\\
\hline
Regression&$`r anova.24[1,2]`$&$`r anova.24[1,1]`$&$`r anova.24[1,3]`$&$\sigma^2+\beta_1^2`r sum((Copiers-mean(Copiers))^2)`$\\
Error&$`r anova.24[2,2]`$&$`r anova.24[2,1]`$&$`r anova.24[2,3]`$&$\sigma^2$\\
\hline
Total&$`r anova.24[2,2]+anova.24[1,2]`$&$`r anova.24[1,1]+anova.24[2,1]`$&\\
\hline
Correction for mean&$`r length(Copiers)+mean(Copiers)^2`$&$`r length(Copiers)`$&\\
Total, uncorrected&$`r sum(Copiers^2)`$&1&\\
\end{tabularx}

This table has two extra rows; they are a decomposition of the total sum of scares into a simple sum of squares ("total uncorrected sum of squares" ,$\sum_i Y_i^2$) and the number of observations times the mean squared ("correction for the mean sum of squares", $n\bar{Y}^2$)

###### b.
```{r}
alpha.24 = .10
F.crit = qf(1-alpha.24,df1 = 1,df2 = model5$df)
F.24 = anova.24[1,4]
detach(data5)
```

Our test is as follows:
\begin{center}
\begin{tabular}{c r}
H$_0$:&$\beta_1 =0$\\
H$_a$:&$\beta_1 \neq0$\\
\end{tabular}

\end{center}
The Decision Rule is:
\begin{tabular}{c c c}
if& $F^*\leq F(1-`r alpha.24`,1, `r model5$df`),$& Conclude $H_0$\\
if& $F^*> F(1-`r alpha.24`, 1,`r model5$df`),$& Conclude $H_a$\\
\end{tabular}

R says: $F^*=`r F.24` >  `r F.crit` = F(`r 1-alpha.24`,1, `r model5$df`)$, so we reject the null hypothesis and conclude that there is a significant linear association between number of copiers maintenanced and length of service.

###### c.
The total variation is reduced by $`r summ5$r.squared*100`$\%.  This is $R^2$, the coefficient of variation.  It's pretty high, though that really depends on the context; I don't know much about this field.

###### d. 
The coefficient of variation is $r = \sqrt{R^2} = `r sqrt(summ5$r.squared)`$ (choose positive since their is a positive relationship)

##### 2.28.)
Mean Muscle Mass

###### a.
```{r}
alpha.28 = .05
data28 = read.table("CH01PR27.txt")
names(data28)<-c('MMM','AGE')
attach(data28)
model28 <- lm(MMM~AGE)
summ28 <- summary(model28)
conf.int.28 <- predict(model28,data.frame(AGE=60),interval='confidence',level=0.95)
pred.int.28 <- predict(model28,data.frame(AGE=60),interval='prediction',level=0.95)
n= length(MMM)

Xh.28 = 60
Yh.28 = model28$coef[1]+Xh.28*model28$coef[2]
se.Yh.28 = sqrt(((Xh.28-mean(AGE))^2/((n-1)*var(AGE))+1/n)*summ28$sigma^2)
W.28 = sqrt(2*qf(1-alpha.28,2,n-2))

WHSu.28 = Yh.28 + 2*se.Yh.28
WHSl.28 = Yh.28 - 2*se.Yh.28
WHS.int.28 <-c(WHSl.28,WHSu.28)

detach(data28)
```

###### a. 
The 95\% confidence interval is $(`r conf.int.28[2]`,`r conf.int.28[3]`)$.  We can be 95\% confident that the mean muscle mass for women 60 years old is between $`r conf.int.28[2]`$ and $`r conf.int.28[3]`$.

###### b.
The 95\% prediction interval is $(`r pred.int.28[2]`,`r pred.int.28[3]`)$.  We can be 95\% confident that the mean muscle mass for a woman who is 60 years old will be between $`r pred.int.28[2]`$ and $`r pred.int.28[3]`$.  This is wider than the confidence interval since we need to take into account the variation in muscle mass for women aged 60; in the confidence interval, the only variation we account for is in the regression model.

###### c. 
The confidence band at $X_h=60$ (the interval) is $(`r WHS.int.28[1]`,`r WHS.int.28[2]`)$, just slightly wider than the confidence interval in part a.  We expect it to be wider because the factor $W$ in the formula must take into account the entire regression line, whereas the critical $t$ value in the confidence interval needs only to take into account the single point.  It is very close in this case because the value $X_h = 60$ is very close to the mean of the $X_i$, $\bar{X} = `r mean(data28[,2])`$.

##### 2.34.)
Grade point Average

###### a.
Random Variables.  Scores on the ACT usually change when a student re-takes the test; there is inherent variability of the score for each student.  Also, the study was designed by selecting 120 raondom students, then observing the test scores and the GPA at the end of freshman year; two random variables from each subject.  If the students were chosen according to ACT score, then I think it would make more sense to treat it as known constants.

##### 2.42.)
Property Assessments

###### a.
```{r}
data42 <- read.table(file = "CH02PR42.txt")
names(data42) <-c('X.42','Y.42')
attach(data42)
plot(x=X.42,y=Y.42)
```

###### b.
```{r}
r12 = cor(X.42,Y.42)
```

$r_{12}$ is an estimator of the correlation.  The estimate is $r = `r r12`$.

###### c.
```{r}
alpha42c <- .05
n42 = length(X.42)
df42 = n42-2
tval42c <- qt(1-alpha42c,df42)
tstar42c<- r12*sqrt(df42)/sqrt(1-r12^2)
P42c <- 2*pt(-tstar42c,df42)
```

Our test is as follows:
\begin{center}
\begin{tabular}{c r}
H$_0$:&$\rho_{12} = 0$\\
H$_a$:&$\rho_{12} \neq 0$\\
\end{tabular}

\end{center}
The Decision Rule is:
\begin{tabular}{c c c}
if& $|t^*|\leq t(`r 1-alpha42c/2`, `r df42`),$& Conclude $H_0$\\
if& $|t^*|> t(`r alpha42c/2`, `r df42`),$& Conclude $H_a$\\
\end{tabular}

For $\alpha = `r alpha42c`$, the critical t-value for the one-sided test is $t(`r 1-alpha42c`,`r df42`) = `r tval42c`$.  R gives $t^*=`r tstar42c`$.  We reject the null hypothesis that $\rho_{12} = 0$; there is insufficient evidence to conclude that the time to repair an additional copier requires on average less than 14 minutes of extra repair time.  The $P$-value is $`r P42c`$.

`r detach(data42)`
##### 3.3)
GPA and ACT scores

###### b.
```{r}
data3 <- read.table("CH01PR19.txt")
names(data3) <- c("GPA","ACT")
attach(data3)
model3 <- lm(data3)

dotchart(model3$residuals,
  main = 'Dotplot of Residuals',
  xlab = 'Residual',
  ylab = 'Subject ID')
```

No clear patterns, we do have a few points that are potential outliers.

###### c.
```{r}
Yhat3 <- predict(model3,data=data3&ACT)
plot(x=Yhat3,y=model3$residuals,
  main = 'Residual Plot',
  xlab = 'Predicted GPA',
  ylab = 'Residua')
abline(0,0)
```

Again, I wouldn't want to commit to any observed pattern.  *Maybe* the variance is a bit larger beyond $\hat{Y}=3.2$, and a bit smaller around $\hat{Y} = 3.0$.

###### d. 
```{r}
qqnorm(model3$residuals,
  datax=FALSE,
  xlab='Theoretical Quantiles',
  ylab='Sample Quantiles')
abline(0,sd(model3$residuals))
ei3 <- model3$residuals
ps3 <- ppoints(length(ei3))
qs3 <- qnorm(ps3)
r3 <- cor(sort(ei3),qs3)
alpha3d <- .05
```

Our test is as follows:
\begin{center}
\begin{tabular}{c r}
H$_0$:&$\rho = 1$\\
H$_a$:&$\rho \neq 1$\\
\end{tabular}

\end{center}
The Decision Rule is:
\begin{tabular}{c c c}
if& $r\geq 0.987$& Conclude $H_0$\\
if& $r<0.987,$& Conclude $H_a$\\
\end{tabular}

For $\alpha = `r alpha3d`$, the critical value of the correlation is larger than 0.987 (table only goes up to $n=100$, we have $n=120$).  R gives $r = `r r3`$.  We reject the null hypothesis that $\rho_{12} = 1$; there is sufficient evidence to conclude that there is a significant departure from normality in the distribution of the residuals.  There is significant evidence to conclude that the error in the linear model for predicting GPA from ACT score is non-normal.

  
###### e.
`r require(lawstat)`
```{r}
BF3.test <- levene.test(ei3[order(ACT)],group=c(rep(1,sum(ACT<26)),rep(2,sum(ACT>=26))))
alpha3e <- .01
tval3e = qt(1-alpha3e/2,length(ei3)-2)
```

Our test is as follows:
\begin{center}
\begin{tabular}{c r}
H$_0$:&Mean deviation of residuals from median of low group = mean deviation of residuals from median of high group (variance homogeneity)\\
H$_a$:&Mean deviations of residuals from low and high groups are different (variance heterogeneity)\\
\end{tabular}

\end{center}
The Decision Rule is:
\begin{tabular}{c c c}
if& $|t^*_{BF}|\leq t(1-\alpha/2;n-2)$,& Conclude $H_0$\\
if& $|t^*_{BF}|>t(1-\alpha/2;n-2),$,& Conclude $H_a$\\
\end{tabular}

For $\alpha = `r alpha3e`$, the critical t-value is `r tval3e`.  R gives $t^* =`r sqrt(BF3.test$statistic)`$.  We reject the null hypothesis that the variance is constant.  There is significant evidence that the variance in the simple linear model predicting GPA from ACT score is nonconstant.  This supports the preliminary findings from part (c), and the feeling overall that this data is not that well fit by a simple linear model.

`r detach(data3)`

##### 3.7.)
Muscle Mass

###### a.
```{r}
data7 <- read.table("CH01PR27.txt")
names(data7) <- c("MM","AGE")
attach(data7)
stem(AGE)
```

Looks like a uniform random sample of age to me.  It's not perfectly flat, but then we shouldn't expect it to be.  No clear patterns.  Maybe we should be worried to ahve so many women in their upper 80s but I'd say it's pretty consistent.

###### b.
```{r}
model7 <- lm(MM~AGE)
ei7 <- resid(model7)
dotchart(ei7,
  main = 'Dotplot of Residuals',
  xlab = 'Residual',
  ylab = 'Subject ID')
```

There seems to be at least one outlier, otherwise the residuals seem to pretty randomly distributed.

###### c.
```{r}
Yhat7 = predict(model7,newdata = data.frame(AGE))
plot(x=Yhat7,y=ei7,
     main = 'Scatterplot of Residuals vs. Predicted Mean Muscle Mass',
     xlab = 'Predicted Mean Muscle Mass',
     ylab = 'Residual')

plot(x=AGE,y=ei7,
     main = 'Scatterplot of Residuals vs. Age',
     xlab = 'Age',
     ylab = 'Residual')

```

Since $\hat{Y}$ is simply a linear function of age, their is no new information you can get from one plot that isn't already contained in the ther (assuming we know the regression equation).  For instance: from the first plot I can see that when the predicted muscle mass is low, the residuals have more spread (a departure from the assumption that the error terms have constance variance).  However, since I know that the linear model has a negative slope, I can deduce from that fact that women with high age will probably have more variability in their mean muscle mass.  That is, the overall trend in the residuals is the same in both plots, except for a reflection.  The x-axis has simply been transformed.  Another potential dparture is the slight bow in the data at a mean muscle mass of about 85, and an age of about 60.

###### d.
```{r}
qqnorm(ei7)
abline(0,sd(ei7))
shapiro.test(ei7)
```

As we can see, since the $P>\alpha = 0.10$, we fail to reject the null hypothesis, so there is significant evidence to conclude that the residuals in the linear mdoel are normally distributed.

###### e.

```{r}
BF7.test <- levene.test(ei7[order(AGE)],group=c(rep(1,30),rep(2,30)),location = 'median')
alpha7e = .01
tval7e = qt(1-alpha7e/2,length(ei7)-2)
```

Our test is as follows:
\begin{center}
\begin{tabularx}{\textwidth}{c X}
H$_0$:&Mean deviation of residuals from median of low group = mean deviation of residuals from median of high group (variance homogeneity)\\
H$_a$:&Mean deviations of residuals from low and high groups are different (variance heterogeneity)\\
\end{tabularx}

\end{center}
The Decision Rule is:
\begin{tabular}{c c c}
if& $|t^*_{BF}|\leq t(1-\alpha/2;n-2)$,& Conclude $H_0$\\
if& $|t^*_{BF}|>t(1-\alpha/2;n-2),$,& Conclude $H_a$\\
\end{tabular}

For $\alpha = `r alpha7e`$, the critical t-value is $`r tval7e`$.  R gives $t^* =`r sqrt(BF7.test$statistic)`$ (also, we have that $p=`r BF7.test[2]`>`r alpha7e`).  We fail to reject the null hypothesis that the variance is constant.  There is not sufficient evidence to show that the variance in the simple linear model predicting Mean Muscle Mass from age is significantly nonconstant.  This does not support my preliminary feelings from c.



`r detach(data7)`


##### 3.13.)
Copier maintenance

###### a.
Our test is as follows:
\begin{center}
\begin{tabular}{c r}
H$_0$:&$\mathbbm{E}[Y]=\beta_0+\beta_1x$\\
H$_a$:&$\mathbbm{E}[Y]\neq\beta_0+\beta_1x$\\
\end{tabular}
\end{center}
That is, the null hypothesis states that the mean of service time at each level of $X$ (number of copiers) is linearly related to the number of copiers.

###### b.
```{r}
data13 <- read.table("CH01PR20.txt")
#Fit the reduced model
names(data13) <- c("Time","Copiers")
attach(data13)
model13.lm = lm(Time~Copiers)
#Fit the full model
model13.fm = lm(Time~factor(Copiers))
anova(model13.lm,model13.fm)
detach(data13)
```

The above table gives us the statistic and more importantly, the $P$ value for the lack of fit test.  Since we have that $P = 0.4766>0.05=\alpha$, we fail to reject the null hypothesis.  There is not sufficient evidence to conclude that the Copier maintenance data are significantly poorly fitted by the linear model.
  There is not a significant lack of fit.

##### 3.17.)
Sales growth

###### a.
```{r}
data17 <- read.table("CH03PR17.txt")
names(data17) <- c("Sales","Year")
attach(data17)
plot(x=Year,y=Sales,
      main = 'Sales vs Coded Year',
      ylab = "Sales ($1000's)",
      xlab = "Year (coded")
```

The linear relation seems fairly strong throughthe first 8 data ponts, however, the last two (Year=8,9) seem to buck that trend just a little bit and a model that allows a slight curvature would fit much better than a linear model.

###### b. 
```{r}
library(MASS)
model17.lm <- lm(Sales~Year)
model17.bc <- boxcox(model17.lm,
                     lambda = seq(.3,.7,.1),
                     interp = F)
```

The Log-Likelihood is maximized ath $\lambda=0.5$, so the transformation $Y\mapsto\sqrt{Y}$ is suggested.

###### c.
```{r}
Sales.sqrt <- sqrt(Sales)
model17.lm.txfm <- lm(Sales.sqrt~Year)
```

The estimated regression function is $\mathbbm{E}[\sqrt{Y}] = `r model17.lm.txfm$coefficients[1]` + `r model17.lm.txfm$coefficients[2]`x$.

###### d. 
```{r}
plot(x=Year,
     y=Sales.sqrt,
     main = 'Transformed Scatterplot',
     xlab = 'Coded Year',
     ylab = 'Sqrt(Sales)')
abline(model17.lm.txfm)
```

The regression function seems to be a good fit to the transformed data.

###### e.
```{r}
ei17 <- resid(model17.lm.txfm)
Yhat17 <- predict(model17.lm.txfm,newdata = data.frame(Year))
plot(x=Yhat17,y=ei17,
      main = 'Residuals vs. Predicted transformed sales value')
abline(0,0)
qqnorm(ei17)
abline(0,sd(ei17))
```

Both the residual plot and the normal Q-Q plot show a pretty good fit to the data.  The residual plot shows a random scatter about 0 with no clear patterns, while the normal Q-Q plot shows a pretty good fit to the line throught he origin.

###### f.
We have:
\[
\mathbbm{E}[\sqrt{Y}] = `r model17.lm.txfm$coefficients[1]` + `r model17.lm.txfm$coefficients[2]`X
\],
that is, 
\[
\sqrt{\hat{Y_h}} = `r model17.lm.txfm$coefficients[1]` + `r model17.lm.txfm$coefficients[2]`X_h
\]
So, squaring both sides gives:
\[
\hat{Y_h} = (`r model17.lm.txfm$coefficients[1]` + `r model17.lm.txfm$coefficients[2]`X_h)^2
\]

##### 3.20.)
For the transofrmation $X'=1/X$, the simple linear model becomes the following:
\[
Y = \beta_0+\beta_1\frac{1}{X} + \varepsilon
\]
So the error terms do not change.  When we make the transformation $Y' = 1/Y$, the simple linear model is:
\[
Y' = \frac{1}{Y} = \beta_0+\beta_1X+\varepsilon
\]
Since $\beta_i, X$ are constants, we could re-write as
\[
Y' = \varepsilon^*
\]
where $\varepsilon^* \sim N(\beta_0+\beta_1X,\sigma^2)$.  Now, the error terms for the transformed model are, by assumption, the same since we just have a simple linear model for $1/Y$.  However, in terms of the original variable, $Y$, the model itself has changed quite profoundly:
\[
Y = \frac{1}{\varepsilon^*}=\frac{1}{\beta_0+\beta_1X+\varepsilon}
\]
So $Y$ now has a reciprocal normal distribution and the error term is no longer additive - it is no longer a straightforward matter to determine the effect of the error term on the original response variable (besides just writing down the above equation).  Note that $Y$ is now distributed as a reciprocal normal random variable and has no finite moments, and we will probably run into trouble if $Y'$ is close to 0.

##### 3.24
Blood presure.

###### a.
```{r}
data24 <- read.table("CH03PR24.txt")
names(data24) <- c("dBP","Age")
attach(data24)
model24 <- lm(dBP~Age)
beta1.24 <- model24$coefficients[2]
beta0.24 <- model24$coefficients[1]
ei24 <- resid(model24)
plot(x=Age,
     y=ei24,
     main = "Residuals against Age",
     xlab = "Age",
     ylab = "Residual")
abline(0,0)
```

The regression function is 
\[
\mathbbm{E}[Y] =`r beta0.24` + `r beta1.24` X 
\]
The residual plot shows that we have a large outlier and perhaps a slight bow to the data.

###### b.
```{r}
data24b <- data24[-7,]
names(data24b) <- c('dBP','Age')
detach(data24)
attach(data24b)
model24b <- lm(dBP ~ Age)
beta0.24b <- model24b$coefficients[1]
beta1.24b <- model24b$coefficients[2]
```

The modified regression function is
\[
\mathbbm{E}[Y] =`r beta0.24` + `r beta1.24` X 
\]
Data point 7 had a very large effect on both parameters in the regression equation.  It raised the slope significantly compared to the model with data point 7 omitted.

###### c.
```{r}
pred.interval24c <- predict(model24b,
                            data.frame(Age=12),
                            interval= "pred",
                            level = 0.99)
```

The prediction interval for $X = 12$ is $(`r pred.interval24c[2]`,`r pred.interval24c[3]`)$.  Since observation $Y_7$ falls outside that range, there is a less than 1% chance of seeing a value that far off of the regression line.  Somethng may be wrong with that data point, or we just need to collect more data.