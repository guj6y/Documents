\documentclass[11pt]{amsart}
\usepackage{fullpage}
\usepackage{amsmath,calc,hyperref,xcolor,tikz,pgfplots,array,tabularx,caption,pgfplotstable,epstopdf,listings,empheq}
\pgfplotsset{compat=newest}
\listfiles
\newlength\dlf
\newcommand\alignedbox[2]{
  % #1 = before alignment
  % #2 = after alignment
  &
  \begingroup
  \settowidth\dlf{$\displaystyle #1$}
  \addtolength\dlf{\fboxsep+\fboxrule}
  \hspace{-\dlf}
  \boxed{#1 #2}
  \endgroup
}
\newcommand\Ex[0]{\mathbb{E}_x}
%\date{}                                           % Activate to display a given date or no date
%%\begin{tikzpicture}
%%\begin{axis}[axis lines=middle, 
%%xmin = -3, xmax = 3,
%%ymin = -1, ymax = 6,
%%thick,
%%xlabel = $x$,ylabel = $y$,
%%samples = 200,legend pos = north west,
%%ytick = {-4,2,4}, xtick = {-3,-1,1,3},x=0.7cm,y=0.6cm,
%%]
%%\addplot [very thick,domain=-3:3,red] (x,{abs(x^4-4*x^2)});
%%\legend{$h(x)$,$h(3x)$,$h(\frac{1}{3}x$}
%%\end{axis}
%%\end{tikzpicture}
 

\begin{document}
\sffamily
\section*{Ecology 518: Homework 5\\ Nick Kappler}
\subsection*{C. Deriving the System-wide Dynamical Equation}
We assume now that $N_{j,x}(t)$ and $N_{m,x}(t)$ are independently distributed in space ($x$) according to the negative binomial distribution.  We want a dynamical equation for $\overline{N_j}(t)$, which, by symmetry, will be identical to the dynamical equation for $\overline{N_m}(t)$.  Let also $\Ex[N_{j,x}(t)] = \overline{N_{j}}(t)$.  Then, 
\begin{align*}
\Ex[N_{j,x}(t+1)] &= \Ex[\lambda_{j,x}(t)N_{j,x}(t)]\\
&=\Ex[Rs_jN_{j,x}(t)e^{-\alpha(N_{j,x}(t) + N_{m,x}(t))}]\\
&=Rs_j\Ex[N_{j,x}(t)e^{-\alpha N_{j,x}(t)}e^{-\alpha N_{m,x}(t)}]\\
\end{align*}
Now, since $N_{j,x}(t)$ and $N_{m,x}(t)$ are independent, we can split the expectation of a product into a product of expectations:
\[
\Ex[N_{j,x}(t+1)] =Rs_j\Ex[N_{j,x}(t)e^{-\alpha N_{j,x}(t)}]\Ex[e^{-\alpha N_{m,x}(t)}]
\]

Notice that $\Ex[e^{-\alpha N_{m,x}(t)}]$ is simply the Laplace transform (with parameter $\alpha$) of a negative binomial random variable.  From the laplace transform notes we find the laplace transform of a negative binomial random variable:
\[
\Ex[e^{-\alpha N_{m,x}(t)}]=\varphi(\alpha)=\left(\frac{1}{1+\mu(1-e^{-\alpha})/k}\right)^k
\]
where $\mu$ is the mean and $k$ the skew parameter of the negative binomial.  $k$ is not specified, however, we konw that $\mu$ must be the spatial average of species $m$, so substitute $\mu \to \overline{N_m}(t)$:
\begin{align*}
\Ex[e^{-\alpha N_{m,x}(t)}] &= \left(\frac{1}{1+\overline{N_m}(t)(1-e^{-\alpha})/k}\right)^k\\
&= \left(\frac{k}{k+\overline{N_m}(t)(1-e^{-\alpha})}\right)^k\\
&= \left(\frac{k+\overline{N_m}(t)(1-e^{-\alpha})}{k}\right)^{-k}\\
&= \left(1+\frac{\overline{N_m}(t)(1-e^{-\alpha})}{k}\right)^{-k}\\
\end{align*}
Now, $\Ex[N_{j,x}(t)e^{-\alpha N_{j,k}(t)}]$ is harder to deal with but notice that formally we have:
\begin{align*}
\frac{d}{d\alpha}\varphi(\alpha) &= \frac{d}{d\alpha}\Ex[e^{-\alpha N_{j,x}(t)}]\\
&=\Ex\left[\frac{d}{d\alpha}e^{-\alpha N_{j,x}(t)}\right]\\
&=\Ex\left[-N_{j,x}(t)e^{-\alpha N_{j,x}(t)}\right]\\
\end{align*}
So, calculating $\Ex[N_{j,x}(t)e^{-\alpha N_{j,k}(t)}]$ is equivalent to calculating $-\frac{d}{d\alpha}\varphi(\alpha)$,\footnote{This can be shown, like you said in class, by appealing to the dominated convergence theorem and using the fact that $0<e^{-\alpha N_{x,j}(t)}<1$. It's straightforward: the integral of the derivative of the integrand will be defined and consistent with what we want provided the integral of the absolute value of the derivative of the integrand is defined.  That is, $\frac{d}{d\alpha}\mathbb{E}[e^{-\alpha N_{x,j}(t)}] = \mathbb{E}[\frac{d}{d\alpha}e^{-\alpha N_{x,j}(t)}]$ provided $\mathbb{E}[|\frac{d}{d\alpha}e^{-\alpha N_{x,j}(t)}|]<\infty$.  Since the exponential term is strictly between $0$ and $1$, that expectation is bounded by $1$, so we are allowed to exchange integration (expectation) and differentiation.. at least I'm pretty sure that's how it goes.  I haven't had to do much analysis the last few years and it's all a bit jumbled in my head.  If push comes to shove I could always show DCT for the difference quotient.} a straightforward application of the chain rule from calculus:
\begin{align*}
-\frac{d}{d\alpha}\varphi(\alpha)&=-\frac{d}{d\alpha}\left(1+\frac{\overline{N_{j}}(t)(1-e^{-\alpha})}{k}\right)^{-k}\\
&=-(-k)\left(1+\frac{\overline{N_{j}}(t)(1-e^{-\alpha})}{k}\right)^{-k-1}\frac{\overline{N_j}(t)e^{-\alpha}}{k}\\
&=\overline{N_j}(t)e^{-\alpha}\left(1+\frac{\overline{N_{j}}N_{j,x}(t+1)(t)(1-e^{-\alpha})}{k}\right)^{-k-1}
\end{align*}
So we now have
\[
\Ex[N_{j,x}(t+1)]=\overline{N_j}(t)Rs_je^{-\alpha}\left(1+\frac{\overline{N_{j}}(t)(1-e^{-\alpha})}{k}\right)^{-k-1}\left(1+\frac{\overline{N_m}(t)(1-e^{-\alpha})}{k}\right)^{-k}
\]
Or with the overline notation:
\[
\overline{N_{j}}(t+1)=\overline{N_j}(t)Rs_je^{-\alpha}\left(1+\frac{\overline{N_j}(t)(1-e^{-\alpha})}{k}\right)^{-k-1}\left(1+\frac{\overline{N_m}(t)(1-e^{-\alpha})}{k}\right)^{-k}
\]
And since
\[
\overline{N_{j}}(t+1)=\overline{N_j}(t)\widetilde{\lambda_j}
\]
simply factoring out an $\overline{N_j}(t)$ from above gives
\[
\widetilde{\lambda_j} = Rs_je^{-\alpha}\left(1+\frac{\overline{N_{j}}(t)(1-e^{-\alpha})}{k}\right)^{-k-1}\left(1+\frac{\overline{N_m}(t)(1-e^{-\alpha})}{k}\right)^{-k}
\]
as dsired.
The dynamical equation for the entire system is given above and reproduced here:
\[
\overline{N_{j}}(t+1)=\overline{N_j}(t)Rs_je^{-\alpha}\left(1+\frac{\overline{N_{j}}(t)(1-e^{-\alpha})}{k}\right)^{-k-1}\left(1+\frac{\overline{N_m}(t)(1-e^{-\alpha})}{k}\right)^{-k}
\]

\end{document}